{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitoraugusto1993/Coursera-courses/blob/main/LLM/Hugging%20Face/Fine%20Tuning/Twitter%20Sentiment%20Analysis/LoRA_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jycxAE8Xfum"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pandas numpy transformers peft datasets evaluate torch accelerate>=0.26.0 psutil mlflow==3.1.1 mlflow-skinny==3.1.1 nltk pandas==2.2.2 numpy==2.0 uuid BitsAndBytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35maarDEV3gh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import mlflow\n",
        "import evaluate\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers.integrations import MLflowCallback\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import random # Random module for generating random numbers and selections\n",
        "import nltk\n",
        "nltk.download('wordnet') # NLTK's WordNet corpus for finding synonyms\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKUsqM4Ewi4h"
      },
      "outputs": [],
      "source": [
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-j8ZfpQwqWj"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "from pyngrok import ngrok\n",
        "from getpass import getpass\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "# ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "# NGROK_AUTH_TOKEN = getpass('Enter the ngrok authtoken: ')\n",
        "ngrok.set_auth_token('33ErQ33bYRKROHRmL4nPUnUVlXN_6Pr3nWEtBMuxD8wkbsEUw')\n",
        "\n",
        "# O pen an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "# 33ErQ33bYRKROHRmL4nPUnUVlXN_6Pr3nWEtBMuxD8wkbsEUw\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQG4t_OwV3gi"
      },
      "source": [
        "## Step 1: Import data set\n",
        "\n",
        "Import dataset from Hugging Face Hub. This Dataset contains thousands of tweets classified as Negative (0) and Positive (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvoxbHf_V3gj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# data = load_dataset(\"gxb912/large-twitter-tweets-sentiment\")\n",
        "\n",
        "data = datasets.load_dataset('sayanroy058/Twitter_Tweets_Mental_Pressure_Sentiment_Dataset')\n",
        "data = data.remove_columns(column_names=['textID','selected_text'])\n",
        "dataset = data['train'].to_pandas()\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qyd2BlV3gj"
      },
      "source": [
        "## Step 2: Clean the text\n",
        "\n",
        "This step is cleaning the raw text data to remove unnecessary characters, such as URLs, special symbols, or HTML tags, and to normalize the text by converting it to lowercase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXmrly5kV3gj"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    text = str(text).lower() # Convert all text to lowercase for uniformity\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
        "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
        "    text = re.sub(r'@\\S+ ', '', text) # Remove any user account mention (e.g: @user)\n",
        "    # text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
        "    return text # Return the cleaned text\n",
        "\n",
        "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
        "# Apply the cleaning function to each row of the 'text' column\n",
        "dataset['cleaned_text'] = dataset['text'].apply(clean_text)\n",
        "\n",
        "# Map sentiment into numerical categories\n",
        "categories = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
        "dataset['Labels'] = dataset['sentiment'].map(categories)\n",
        "\n",
        "# Remove un-necessary columns\n",
        "dataset = dataset.drop(['text','sentiment'], axis=1)\n",
        "\n",
        "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0o7sDpV3gj"
      },
      "source": [
        "## Step 3: Handle missing data\n",
        "\n",
        "Check for empty entries in the column 'Text'. If there are missing data, we should delete the entire rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t19WM_cHV3gj"
      },
      "outputs": [],
      "source": [
        "# Print the count of missing values for each column\n",
        "print(dataset.isnull().sum())\n",
        "print('--------------------')\n",
        "\n",
        "# Remove rows with missing data in the 'text' column\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "# Confirmation of no missing data in the dataset\n",
        "print(dataset.isnull().sum())\n",
        "print('--------------------')\n",
        "\n",
        "# Print the count of entries in the dataset\n",
        "print(dataset.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB1R9Q2aV3gk"
      },
      "source": [
        "## Step 4: Data augmentation\n",
        "\n",
        "In certain cases, especially when data is limited, data augmentation techniques can be applied to generate new training examples by modifying the original dataset.\n",
        "\n",
        "- Paraphrasing: rewriting sentences in different ways while preserving the meaning\n",
        "\n",
        "- Backtranslation: translating text into another language and back again to create variation\n",
        "\n",
        "- Synonym replacement: replacing certain words in the text with their synonyms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2MiZVH3V3gk"
      },
      "outputs": [],
      "source": [
        "dataset.groupby('Labels')['cleaned_text'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzHKXLEEV3gk"
      },
      "outputs": [],
      "source": [
        "# Define a function to find and replace a word with a synonym\n",
        "def synonym_replacement(word):\n",
        "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
        "    synonyms = wordnet.synsets(word)\n",
        "\n",
        "    # If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
        "    # Select a random synonym and get the first lemma (word form) of that synonym\n",
        "    # If no synonyms are found, return the original word\n",
        "    if synonyms:\n",
        "        return random.choice(synonyms).lemmas()[0].name()\n",
        "    return word\n",
        "\n",
        "# Define a function to augment text by replacing words with synonyms randomly\n",
        "def augment_text(example, alpha):\n",
        "    # Split the input text into individual words\n",
        "    words = example[0].split() # Split the input text into individual words\n",
        "\n",
        "    if random.random() > (1 - alpha):\n",
        "        # Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
        "        augmented_words = [synonym_replacement(word) if random.random() > 0.8 else word\n",
        "                            # If random condition met, replace\n",
        "                            for word in words] # Iterate over each word in the original text\n",
        "\n",
        "        # Join the augmented words back into a single string and return it\n",
        "        return pd.Series([' '.join(augmented_words), int(example[1])])\n",
        "    else:\n",
        "        return pd.Series([None, None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3cbGzyWV3gk"
      },
      "outputs": [],
      "source": [
        "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
        "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
        "# Apply about 40% augmentation on samples labeled as Negative (1) and include augmented examples in the original dataset\n",
        "dataset_aug = dataset[dataset['Labels'] == 1].apply(augment_text, alpha=0.4, axis=1)\n",
        "dataset_aug.columns=['cleaned_text','Labels']\n",
        "dataset_aug = dataset_aug.dropna()\n",
        "dataset = pd.concat([dataset, dataset_aug], axis=0, ignore_index=True)\n",
        "# Apply about 30% augmentation on samples labeled as Positive (2) and include augmented examples in the original dataset\n",
        "dataset_aug = dataset[dataset['Labels'] == 2].apply(augment_text, alpha=0.3, axis=1)\n",
        "dataset_aug.columns=['cleaned_text','Labels']\n",
        "dataset_aug = dataset_aug.dropna()\n",
        "dataset = pd.concat([dataset, dataset_aug], axis=0, ignore_index=True)\n",
        "\n",
        "dataset['Labels'] = dataset['Labels'].astype(int)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKVenbw9V3gk"
      },
      "outputs": [],
      "source": [
        "dataset.groupby('Labels')['cleaned_text'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rugUgToV3gk"
      },
      "outputs": [],
      "source": [
        "# Visually check if data is balanced\n",
        "dataset[\"Labels\"].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NpQ4uOV3gk"
      },
      "source": [
        "## Step 5: Tokenization\n",
        "\n",
        "After cleaning the text, we tokenize it. Tokenization splits the text into individual words or subwords that can be used by the model. We will use the BERT tokenizer to ensure compatibility with the Brie-trained model you are fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices = random.sample(range(len(dataset)), 70000)\n",
        "mini_dataset = Dataset.from_pandas(dataset).select(random_indices)\n",
        "mini_dataset = Dataset.to_pandas(mini_dataset)\n",
        "mini_dataset.groupby('Labels')['cleaned_text'].count()"
      ],
      "metadata": {
        "id": "oehH8kFmNRnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQWPpSLPV3gl"
      },
      "outputs": [],
      "source": [
        "# Load the BERT tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# dataset_ds = Dataset.from_pandas(mini_dataset)\n",
        "dataset_ds = Dataset.from_pandas(dataset)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['cleaned_text'], truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset_ds.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipGzltbSV3gl"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.remove_columns(['cleaned_text'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column('Labels', 'labels')\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "tokenized_dataset.column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sE2Gp7zV3gl"
      },
      "source": [
        "## Step 6: Structure the data for fine-tuning\n",
        "\n",
        "You can fine-tune your model once the dataset is cleaned and tokenized. The next step is structuring the data for fine-tuning.\n",
        "\n",
        "First we will split our dataset into three separate datasets: training, validation and test. For this case, we will use 70% of the whole dataset to train de model, 15% to validade and 15% to test. Then, we need to convert our pandas dataframes back to Datasets objects. With our dataset splited and converted, we have to remove an extra column that these steps added ('__index_level_0__')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5KfmDj2V3gl"
      },
      "outputs": [],
      "source": [
        "tokenized_df = tokenized_dataset.to_pandas()\n",
        "\n",
        "train_df, val_test_df = train_test_split(\n",
        "    tokenized_df, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    val_test_df, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
        "val_dataset = val_dataset.remove_columns(['__index_level_0__'])\n",
        "test_dataset = test_dataset.remove_columns(['__index_level_0__'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0wuNpyV3gl"
      },
      "source": [
        "When we load a dataset using the load_dataset from library datasets, they are presented as a DatasetDict object. Sometimes the dataset is already splited into train, validation and test. So, in the next step, we will create a DatasetDict object just like the ones we get when me use load_data function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2BnhjC7V3gl"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "tweet_dataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset, \"test\":test_dataset})\n",
        "tweet_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufc7hj3oV3gm"
      },
      "source": [
        "## Step 7: Create the model\n",
        "\n",
        "Now that we're completely finished with data preprocessing, let's turn to the model. We instantiate it by referencing the pretrained model we want to fine tune. In this case e are using a base DistilBERT model. In this example, we will implement a LoRA (Low-Rank Adaptation) fine-tuning method. LoRA is a parameter-efficient fine-tuning (PEFT) technique that adapts large, pre-trained machine learning models for specific tasks by adding and training only a small number of new parameters. Instead of retraining the entire model, LoRA injects lightweight, low-rank matrices into the original model's layers, freezing the original weights and creating efficient, task-specific adapters. This significantly reduces computational costs and memory requirements, allowing for faster customization of massive models like Large Language Models (LLMs) without compromising performance, and enabling easy switching between different tasks by swapping the adapter weights.\n",
        "\n",
        "How LoRA Works\n",
        "* Freeze the base model: The original, large pre-trained model's weights are kept fixed and are not modified during the adaptation process.\n",
        "* Inject LoRA adapters: Instead of updating the original weight matrices (ΔW) directly, LoRA decomposes the necessary weight update into two smaller, low-rank matrices (A and B).\n",
        "* Train only the adapters: The adaptation process then focuses on training only these smaller A and B matrices.\n",
        "* Merge for inference (optional): For deployment, the trained adapter matrices can be merged back into the base model's weights, creating a single, task-specific model without any added latency.\n",
        "\n",
        "We can simply implement LoRA with the Hugging Face's library PEFT by instantiating a LoraConfig object with the desired parameters. Then, we apply this configuration to the model through the method get_peft_model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvYItJSeTcol"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3,)\n",
        "\n",
        "# Print all model modules to inspect and select the target modules to apply LoRA\n",
        "# for idx, m in enumerate(model.named_modules()):\n",
        "#   print(idx, '->', m)\n",
        "\n",
        "rank = 8\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=rank*2,\n",
        "        target_modules=['v_lin','q_lin','k_lin'],\n",
        "        lora_dropout=0.1,\n",
        "        bias='none',\n",
        "        use_rslora=True,\n",
        "        task_type=\"SEQ_CLS\" # Specify the task type\n",
        "    )\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters() # To see the number of trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBLgxeM4YXT5"
      },
      "source": [
        "An optional, but important thing to do before training is to set a GPU to do the work. If we have access to one, training process can take just a couple minutes instead of several ours compared to CPU. To do this, we define a device we will put our model and our batches on:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a GPU\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "lora_model.to(device)\n",
        "device"
      ],
      "metadata": {
        "id": "cByx1FRpfihS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh2D0m-RV3gm"
      },
      "source": [
        "## Step 8: Training loop configuration\n",
        "\n",
        "The Hugging Face's transformers Trainer Class provides a high level API for training and evaluating PyTorch models, particularly those based on the transformers library. It simplifies the process of setting up and running training loops, handling complexities like distributed training, mixed-precision training, logging and model checkpointing, allowing users to focus on model architecture, datasets and training configuration through the TrainingArguments class.\n",
        "\n",
        "To use the Trainer, you tipically:\n",
        "*   Define your model, dataset, and preprocessor (e.g., tokenizer);\n",
        "*   Create an instance of TrainingArguments to configure your training parameters;\n",
        "*   Instantiate the Trainer with your model, TrainingArguments, datasets, and preprocessor;\n",
        "*   Call the train() method to start the training process.\n",
        "\n",
        "The Trainer streamlines the process of fine-tuning pre-trained transformers models or training new models from scratch, making it a powerful toll for researchers and developers working with NLP and other machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation Metrics\n",
        "\n",
        "By default, the Trainer only tracks the loss function during training and evaluation. To track how the model is performing with the validation data we need to specify an evaluation metric, such as accuracy, recall, precision and F1-score."
      ],
      "metadata": {
        "id": "6ucTg-3-q1OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # For multi-class classification, convert logits to predicted class IDs\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and f1-score.\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    precision = evaluate.load(\"precision\")\n",
        "    recall = evaluate.load(\"recall\")\n",
        "    f1 = evaluate.load(\"f1\")\n",
        "\n",
        "    # 'average' can be 'binary', 'micro', 'macro', 'weighted', or None.\n",
        "    # 'weighted' is often suitable for imbalanced datasets in multi-class.\n",
        "    metrics = {}\n",
        "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
        "    metrics.update(precision.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "    metrics.update(recall.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "    metrics.update(f1.compute(predictions=predictions, references=labels, average='weighted'))\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "XdK7yCYuJ-wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dKZzeXZV3gn"
      },
      "source": [
        "#### Training\n",
        "\n",
        "With the evaluation metrics defined, we can now configure and run the model training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "import mlflow\n",
        "\n",
        "lr = 3e-4\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "\n",
        "mlflow.set_experiment(f\"LoRA_NNP_Twitter_{model_name}\")\n",
        "mlflow.pytorch.autolog()\n",
        "run_id = f\"model-lora-rank-{rank}-lr{str(lr)}-batch-size-{batch_size}-{num_epochs}-epochs-{str(uuid4())[:8]}\"\n",
        "\n",
        "mlflow.end_run()\n",
        "with mlflow.start_run(log_system_metrics=True, run_name=run_id) as run:\n",
        "  training_args = TrainingArguments(        # Step 1: Set training arguments for fine-tuning the model\n",
        "    output_dir='./results',                 # Directory where results will be stored\n",
        "    num_train_epochs=num_epochs,            # Number of epochs (full passes through the dataset)\n",
        "    per_device_train_batch_size=batch_size, # Batch size per GPU/CPU during training\n",
        "    per_device_eval_batch_size=batch_size,  # Batch size per GPU/CPU during evaluation\n",
        "    eval_strategy=\"epoch\",                  # Evaluate the model at the end of each epoch\n",
        "    optim=\"adamw_torch\",                    # Define the optimizer\n",
        "    lr_scheduler_type=\"linear\",             # Define the behavior os the decay of learning rate\n",
        "    learning_rate=lr,                       # Learning rate\n",
        "    weight_decay=0.01,                      # Learning rate decay of the training steps\n",
        "    metric_for_best_model=\"accuracy\",       # Set the metric que trainer will use to evaluate the model\n",
        "    greater_is_better=True,                 # How the trainer should evaluate a model as better or worse than others\n",
        "    load_best_model_at_end=True,            # Load the best model at the end of training\n",
        "    save_strategy='epoch',                  # Save model at the end of an epoch with best evaluation metric\n",
        "    report_to=\"mlflow\"                      # Enable MLflow logging\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(                        # Step 2: Fine-tune only the final classification head (since earlier layers were frozen)\n",
        "    model=lora_model,                            # Pre-trained BERT model with frozen layers\n",
        "    args=training_args,                     # Training arguments\n",
        "    train_dataset=tweet_dataset['train'],   # Training data for fine-tuning\n",
        "    eval_dataset=tweet_dataset['val'],      # Validation data to evaluate performance during training\n",
        "    data_collator=data_collator,            # Data collator to prepare batches of data\n",
        "    processing_class=tokenizer,             # Tokenizer to preprocess the data\n",
        "    compute_metrics=compute_metrics,        # Function to compute evaluation metrics\n",
        "  )\n",
        "\n",
        "  # Step 3: Train the model using PEFT (this performs PEFT because layers were frozen in Step 1)\n",
        "  trainer.train()"
      ],
      "metadata": {
        "id": "aik4K4_kK9Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "id": "s-gYCysBz5LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Evaluation"
      ],
      "metadata": {
        "id": "ryVnDj3qzY-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=tweet_dataset['test'])"
      ],
      "metadata": {
        "id": "QYd3uFnAZ4Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfD-oIMAuMtz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}